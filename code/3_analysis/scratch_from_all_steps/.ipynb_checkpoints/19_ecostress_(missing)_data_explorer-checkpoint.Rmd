---
title: "ecostress_data_explorer"
author: "Anna Boser"
date: "12/22/2021"
output: html_document
---

New goal: 
1. Write a script that resamples, saves, and adds to a CSV for (1) ag and (2) natural all ET and uncertainty measures one at a time, deleting them as you go.  Also it periodically saves the csv just in case it blows up because itâ€™s too big
    1. Function for each date
        1. Read in ET and ETuncert
        2. Resample both
        3. Mask both to (1) ag and (2) natural
        4. Add to the two csvs 
        5. Delete everything except the csvs
        6. Every 50 or so files save the csv


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(here)
library(raster)
library(data.table)
library(dplyr)
library(ggplot2)
library(stringr)
```

```{r}
# get the ET file names
files <- list.files(path = here("data", "raw", "ECOSTRESS"), full.names = TRUE)

# empty consistent grid
CA_grid <- raster(here("data", "intermediate", "CA_grid.tif"))

# ag and natural land rasters
ag <- raster(here("data", "intermediate", "agriculture", "ag_indicator.tif"))
natural <- raster(here("data", "intermediate", "counterf", "counterf_indicator.tif"))
pixels_of_interest <- ag|natural

# full, time invariant grid
full <- fread(here("data", "for_analysis", "full_grid_time_invariant.csv"))

# remove entries that are neither ag nor natural
full <- full[agriculture == 1|counterfactual == 1]
fwrite(full, here("data", "for_analysis", "ag_count_time_invariant.csv"))
# remove the entries that nave na slope etc becasue that just means they're in the ocean (there are only a few)
full <- full[aspect > -20]
```

```{r}
# plot a subset
sample <- full[sample(nrow(full), 10000), ]
ggplot(sample) + 
  geom_point(aes(x = x, y = y, col = soil))
ggplot(sample) + 
  geom_point(aes(x = soil, y = aspect, col = agriculture))
```


```{r}
for (i in 1:26){
  print(plot(raster(files[i])))
}
```

```{r}
# get the different image timestamps
timestamps <- str_extract(files, regex('(?<=_doy)[0-9]*(?=_aid0001.tif)'))
unique_timestamps <- unique(timestamps)
names(timestamps) <- files
```

```{r}
# create a dataset for each timestamp
make_dataset <- function(timestamp){
  
  print(paste("New timestamp:", timestamp))
  
  timestamp_files <- names(timestamps[timestamps == timestamp])
  
  # read in rasters, resample to CA_grid, and turn into a rasterbrick
  read_and_rename <- function(file){
    raster <- raster(file)
    names(raster) <- str_extract(names(raster), regex('(?<=_PT_JPL_)[A-z_]*(?=_doy)'))
    raster <- raster(file) %>% resample(CA_grid, method = "bilinear")
    return(raster)
  }
  ECO_brick <- brick(lapply(timestamp_files, read_and_rename))
  
  rename_cols <- function(name){
    if (str_detect(name, "ETinstUncertainty")){
      newname <- "ET_sd"
    } else {
      newname <- "ET"
    }
    return(newname)
  }
  
  ncol <- length(names(ECO_brick))
  names(ECO_brick) <- sapply(names(ECO_brick), rename_cols)
  
  # mask out non ag or natural
  mask(ECO_brick, pixels_of_interest)

  # convert rasters to dataframe rows
  dataset <- as.data.frame(ECO_brick, xy = TRUE, na.rm=TRUE)
  
  # if the ET_sd column is missing, add it with a bunch of NA values
  if (is.null(dataset$ET_sd)){
    dataset$ET_sd <- NA
  }
  
  # add date labels
  dataset$date <- as.character(as.Date(timestamp, "%Y%j%H%M%S"))
  dataset$hhmmss <- substring(timestamp, 8)
  dataset$year <- year(as.Date(timestamp, "%Y%j%H%M%S"))
  
  #turn into data table for faster rbinding
  dataset <- as.data.table(dataset)

  return(dataset)
}
```

given how bad some of the sds are I might drop pixels where the sd is greater than like 100 later 

```{r}
# run the dataset command for all timestamps
time <- Sys.time()
dataset_list <- lapply(unique_timestamps, make_dataset)
dataset <- rbindlist(dataset_list, fill = TRUE)
print(paste("Time elapsed to build dataset:", Sys.time() - time))
```



