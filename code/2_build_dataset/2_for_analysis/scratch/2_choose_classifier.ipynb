{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4929d2e-7f5a-4be0-aab1-e6a7962f2261",
   "metadata": {},
   "source": [
    "Find your regressor. test the need for a lot of data , different regressors and which months I should consider (forget the winter?)\n",
    "    \n",
    "    1) break dataset into .001, .01, .1. \n",
    "    \n",
    "    2) find the smallest data size to experiment on\n",
    "    \n",
    "    3) tune and compare a variety of classifiers (rf and xgb) on appropriate size of data: make a dataset with columns for classifier, dataset size, and metrics for \n",
    "    \n",
    "        1) overall performance \n",
    "    \n",
    "        2) performance by month (?)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22d3862d-a0ff-45c1-85cc-4caa36c30072",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import lightgbm as lgb\n",
    "from pyprojroot import here\n",
    "import math\n",
    "import gc\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91af8cfa-6fd4-4727-a18b-c41d559d5d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# break dataset into .001, .01, .1. \n",
    "# if you don't have subsamples of the dataset, make them, otherwise, load them\n",
    "\n",
    "outpath = str(here(\"./data/for_analysis/sample/\"))\n",
    "fracs = [.001, .01, .1] #, 1, .0001, .00001] # I should add the full dataset as an option\n",
    "samples = {} #dictionary of subsamples\n",
    "\n",
    "if not os.path.exists(outpath):\n",
    "    os.makedirs(outpath)\n",
    "\n",
    "    # read in dataset\n",
    "    dataset = pd.read_csv(str(here(\"./data/for_analysis/counterfactual.csv\")))\n",
    "    dataset = dataset.query('ET >= 0') # remove missing data\n",
    "    dataset.head()\n",
    "    \n",
    "    for frac in fracs: # make and save subsets for each frac\n",
    "        samples[frac] = dataset.sample(frac = frac)\n",
    "        samples[frac].to_csv(outpath+\"/sample\"+str(frac)+\".csv\", index=False)\n",
    "else:\n",
    "    for frac in fracs: # read in subsets for each frac   \n",
    "        samples[frac] = pd.read_csv(outpath+\"/sample\"+str(frac)+\".csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b8079f4-e47b-4c35-9b7a-8181c58a1c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    frac        r2  r2_score      rmse  frac        r2  r2_score      rmse  \\\n",
      "0  0.001  0.928135  0.928116  4.761597  0.01  0.963025  0.962988  3.472063   \n",
      "\n",
      "   frac        r2  r2_score      rmse  \n",
      "0   0.1  0.983443  0.983433  2.318941  \n"
     ]
    }
   ],
   "source": [
    "# train a rf on all sizes of data. \n",
    "\n",
    "random_split_eval = []\n",
    "\n",
    "for frac in fracs: \n",
    "    \n",
    "    dataset = samples[frac]\n",
    "    \n",
    "    # split between predictors and predicted\n",
    "    X = dataset.iloc[:, 0:(dataset.shape[1]-1)].values # everything, including lat, lon, and date, are predictors. \n",
    "    # I might want to eventually redefine dates as times of year to make the actual year not matter\n",
    "\n",
    "    y = dataset.iloc[:, (dataset.shape[1]-1)].values # Predict ET\n",
    "    # print(X)\n",
    "\n",
    "    # make train test split for a random (not spatial) hold out validation\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0) # random state is for reproducibility to consistently get the same random shuffle\n",
    "\n",
    "    # build the regressor\n",
    "    regressor = RandomForestRegressor(n_estimators=100, random_state=0) # I stick with the default recommended 100 trees in my forest\n",
    "    regressor.fit(X_train, y_train)\n",
    "    \n",
    "    # predict y \n",
    "    y_pred = regressor.predict(X_test)\n",
    "    \n",
    "    # evaluate\n",
    "    random_test = pd.DataFrame({'frac' : [frac], \n",
    "                   'r2' : [np.corrcoef(y_test, y_pred)[0,1]**2],\n",
    "                   'r2_score' : [metrics.r2_score(y_test, y_pred)], \n",
    "                   'rmse' : [np.sqrt(metrics.mean_squared_error(y_test, y_pred))]})\n",
    "    random_split_eval.append(random_test)\n",
    "\n",
    "random_split_eval = pd.concat(random_split_eval, axis=0)\n",
    "print(random_split_eval)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927b9d35-68f2-4c6f-beff-7c2ab7a40858",
   "metadata": {},
   "source": [
    "The metrics do improve with sample size, so the full sample should be used for the final analysis, but for now we use the smallest subsample in order to save on computation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a08abb8b-08c3-4a9e-8333-cba3533d7701",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'samples' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-11dab336607c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# choose the dataset size to continue working with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m.001\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrac\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m.01\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# make even smaller for testing purposes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'samples' is not defined"
     ]
    }
   ],
   "source": [
    "# choose the dataset size to continue working with\n",
    "dataset = samples[.001]\n",
    "# dataset = dataset.sample(frac = .01) # make even smaller for testing purposes\n",
    "del samples\n",
    "\n",
    "# split between predictors and predicted\n",
    "X = dataset.iloc[:, 0:(dataset.shape[1]-1)].values # everything, including lat, lon, and date, are predictors. \n",
    "# I might want to eventually redefine dates as times of year to make the actual year not matter\n",
    "\n",
    "y = dataset.iloc[:, (dataset.shape[1]-1)].values # Predict ET\n",
    "# print(X)\n",
    "\n",
    "# make train test split for a random (not spatial) hold out validation\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0) # random state is for reproducibility to consistently get the same random shuffle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae5b13ef-b0ac-4372-a4b4-6ed2779a6583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define an evaluation function \n",
    "def r2_rmse(g):\n",
    "    r2 = np.corrcoef(g['ET'], g['ET_pred'])[0,1]**2\n",
    "    r2_score = metrics.r2_score(g['ET'], g['ET_pred'])\n",
    "    rmse = np.sqrt(metrics.mean_squared_error(g['ET'], g['ET_pred']))\n",
    "    count = g.shape[0]\n",
    "    return pd.Series(dict(r2 = r2, r2_score = r2_score, rmse = rmse, count = count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4413db6a-fe2f-4e52-bcb2-6feb05e8914f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "{'n_estimators': 1366, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'auto', 'max_depth': 100, 'bootstrap': True}\n",
      "         r2  r2_score      rmse    count    fold_type\n",
      "0  0.928648  0.928627  4.744654  23126.0  random_test\n",
      "   monthgroup        r2  r2_score      rmse   count    fold_type\n",
      "0         0.0  0.537561  0.535175  6.244479   941.0  random_test\n",
      "1         1.0  0.815317  0.814258  5.619110  1231.0  random_test\n",
      "2         2.0  0.932552  0.931614  5.307099  2487.0  random_test\n",
      "3         3.0  0.955092  0.954894  1.170514  9726.0  random_test\n",
      "4         4.0  0.850332  0.849614  6.444139  6054.0  random_test\n",
      "5         5.0  0.827679  0.827278  6.406084  2687.0  random_test\n"
     ]
    }
   ],
   "source": [
    "# try to improve the RF by tuning hyperparameters\n",
    "# see: https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74\n",
    "\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 100, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "# Use the random grid to search for best hyperparameters\n",
    "\n",
    "# First create the base model to tune\n",
    "rf = RandomForestRegressor()\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "# Fit the random search model\n",
    "rf_random.fit(X_train, y_train)\n",
    "\n",
    "# We can view the best parameters from fitting the random search\n",
    "print(rf_random.best_params_)\n",
    "\n",
    "# evaluate this improved RF\n",
    "# predict y \n",
    "y_pred = rf_random.predict(X_test)\n",
    "\n",
    "# evaluate\n",
    "# random_test = dict(val_type = 'random_test', \n",
    "#                    r2 = np.corrcoef(y_test, y_pred)[0,1]**2,\n",
    "#                    r2_score = metrics.r2_score(y_test, y_pred), \n",
    "#                    rmse = np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
    "\n",
    "# make a df with monthgroup, y, and pred\n",
    "df_rand_eval = pd.DataFrame({'monthgroup': X_test[:,dataset.columns.get_loc('monthgroup')], \n",
    "                             'ET':y_test, \n",
    "                             'ET_pred': y_pred})\n",
    "\n",
    "# evaluate\n",
    "random_test = pd.DataFrame(r2_rmse(df_rand_eval)).transpose()\n",
    "random_test[\"fold_type\"] = \"random_test\"\n",
    "print(random_test)\n",
    "\n",
    "# evaluate by monthgroup\n",
    "random_test_by_month = df_rand_eval.groupby(['monthgroup']).apply(r2_rmse).reset_index()\n",
    "random_test_by_month[\"fold_type\"] = \"random_test\"\n",
    "print(random_test_by_month)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15697f8f-1aa9-4eb5-bd2a-b6786d3e70a0",
   "metadata": {},
   "source": [
    "Wow it does way better in the growing season where there are fewer clouds. Not surprised... might want to toss the rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "813d2fbb-8229-4243-afaa-35678fa0513e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To do this I first generate an extra column for my dataset called cv_fold which corresponds to its location\n",
    "dataset = dataset.assign(cv_fold = lambda x: x.x.apply(math.floor)*1000 + x.y.apply(math.floor))\n",
    "\n",
    "# crossvalidate and make crossvalidation dataset\n",
    "\n",
    "df = dataset\n",
    "del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5e85943f-5399-48b6-8583-b07d568b0a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training fold 1 of 10.\n",
      "Starting training fold 2 of 10.\n",
      "Starting training fold 3 of 10.\n",
      "Starting training fold 4 of 10.\n",
      "Starting training fold 5 of 10.\n",
      "Starting training fold 6 of 10.\n",
      "Starting training fold 7 of 10.\n",
      "Starting training fold 8 of 10.\n",
      "Starting training fold 9 of 10.\n",
      "Starting training fold 10 of 10.\n",
      "Done!!\n"
     ]
    }
   ],
   "source": [
    "# elect the best model to try validating by location\n",
    "\n",
    "n_fold = len(set(df['cv_fold'])) # set is same as unique function in R\n",
    "kf = GroupKFold(n_fold)\n",
    "split = kf.split(df, groups = df['cv_fold'])\n",
    "\n",
    "cv_df = pd.DataFrame()\n",
    "\n",
    "for i, (train_idx, test_idx) in enumerate(split):\n",
    "    print(f'Starting training fold {i + 1} of {n_fold}.')\n",
    "    _ = gc.collect()\n",
    "\n",
    "    X_train = X[train_idx,:]\n",
    "    X_test = X[test_idx,:]\n",
    "    y_train = y[train_idx]\n",
    "    y_test = y[test_idx]\n",
    "\n",
    "    regressor = RandomForestRegressor(random_state=0) \n",
    "    regressor.set_params(**rf_random.best_params_) # use the parameters from the randomized search\n",
    "    regressor.fit(X_train, y_train)\n",
    "    y_pred = regressor.predict(X_test)\n",
    "\n",
    "    # cv_fold = np.repeat(df.loc[test_idx]['cv_fold'].iloc[0], X_test.shape[0])\n",
    "    df_to_append = pd.DataFrame({# 'cv_fold': cv_fold, \n",
    "                                 'monthgroup': X_test[:,df.columns.get_loc('monthgroup')], \n",
    "                                 'ET': y_test, \n",
    "                                 'ET_pred': y_pred})\n",
    "\n",
    "    cv_df = cv_df.append(df_to_append, ignore_index = True)\n",
    "\n",
    "print(\"Done!!\")\n",
    "\n",
    "# save the full predictions using the spatial CV\n",
    "cv_df.to_csv(str(here(\"./data/for_analysis/sklearn_RF_full_cv_outputs_1x1.csv\")), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7388729b-c4c1-482d-9772-aa7ce732c35b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1a5be50e-65c3-47ec-8432-dec0637b368c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         r2  r2_score          rmse   count    fold_type\n",
      "0  0.727424  0.725608  8.514942e+37  3218.0   spatial_cv\n",
      "0  0.798889  0.793866  7.359809e+37   644.0  random_test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/bplayground/lib/python3.8/site-packages/pyprojroot/pyprojroot.py:51: UserWarning: Path doesn't exist: /Users/annaboser/Documents/GitHub/ET_agriculture/data/for_analysis/sklearn_RF_validation_stats_1x1.csv\n",
      "  warnings.warn(\"Path doesn't exist: {}\".format(path))\n",
      "/opt/anaconda3/envs/bplayground/lib/python3.8/site-packages/pyprojroot/pyprojroot.py:51: UserWarning: Path doesn't exist: /Users/annaboser/Documents/GitHub/ET_agriculture/data/for_analysis/sklearn_RF_test_stats_by_month_1x1.csv\n",
      "  warnings.warn(\"Path doesn't exist: {}\".format(path))\n"
     ]
    }
   ],
   "source": [
    "# evaluate\n",
    "\n",
    "# get r2, rmse, and count by cv_fold\n",
    "cv_stats = cv_df.groupby('cv_fold').apply(r2_rmse).reset_index()\n",
    "\n",
    "# save this df\n",
    "cv_stats.to_csv(str(here(\"./data/for_analysis/sklearn_RF_cv_fold_stats_1x1.csv\")), index=False)\n",
    "\n",
    "# make a df for general stats for both the spatial cv and the random 20% test\n",
    "spatial_cv = pd.DataFrame(r2_rmse(cv_df)).transpose()\n",
    "spatial_cv[\"fold_type\"] = \"spatial_cv\"\n",
    "\n",
    "test_stats = pd.concat([spatial_cv, random_test])\n",
    "print(test_stats)\n",
    "\n",
    "# save this df\n",
    "test_stats.to_csv(str(here(\"./data/for_analysis/sklearn_RF_validation_stats_1x1.csv\")), index=False)\n",
    "\n",
    "# grouped by month, get r2, rmse, and count\n",
    "cv_stats_by_month = cv_df.groupby('monthgroup').apply(r2_rmse).reset_index()\n",
    "cv_stats_by_month[\"fold_type\"] = \"spatial_cv\"\n",
    "\n",
    "# concat\n",
    "test_stats_by_month = pd.concat([cv_stats_by_month, random_test_by_month])\n",
    "print(test_stats_by_month)\n",
    "\n",
    "# save \n",
    "test_stats_by_month.to_csv(str(here(\"./data/for_analysis/sklearn_RF_test_stats_by_month_1x1.csv\")), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f7c5eb-ff22-419c-8ea6-054f2b53987a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73437a26-2d44-467b-bb69-996ad9a53aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# also try lgb maybe? \n",
    "# see: https://www.analyticsvidhya.com/blog/2021/08/complete-guide-on-how-to-use-lightgbm-in-python/\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
